{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3jc_p1eE6xS"
   },
   "outputs": [],
   "source": [
    "# Google Colab에서 노트북을 실행하실 때에는\n",
    "# https://tutorials.pytorch.kr/beginner/colab 를 참고하세요.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61117,
     "status": "ok",
     "timestamp": 1733208889928,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "eyIQlbYiGiaV",
    "outputId": "e78c8de1-54b1-41a1-ad3d-813a95124496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733208889928,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "14qM3FMgGkXB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/rl')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBAdJixUE6xU"
   },
   "source": [
    "# Super Mario Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXhQhQVVE6xV"
   },
   "source": [
    "``` {.sourceCode .bash}\n",
    "%%bash\n",
    "pip install gym-super-mario-bros==7.4.0\n",
    "pip install tensordict==0.3.0\n",
    "pip install torchrl==0.3.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12378,
     "status": "ok",
     "timestamp": 1733208902302,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "VsqOfRx6GLF0",
    "outputId": "6b4230aa-3563-4995-a3e6-64e611827552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym-super-mario-bros==7.4.0\n",
      "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nes-py>=8.1.4 (from gym-super-mario-bros==7.4.0)\n",
      "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.26.4)\n",
      "Collecting pyglet<=1.5.21,>=1.4.0 (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0)\n",
      "  Downloading pyglet-1.5.21-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.66.6)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.1.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
      "Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: nes-py\n",
      "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nes-py: filename=nes_py-8.2.1-cp310-cp310-linux_x86_64.whl size=535719 sha256=70b2230cf54fdc371f96e944831b12b39f1db100bed09f755b4f8bc23f097c49\n",
      "  Stored in directory: /root/.cache/pip/wheels/34/a7/d5/9aa14b15df740a53d41f702e4c795731b6c4da7925deb8476c\n",
      "Successfully built nes-py\n",
      "Installing collected packages: pyglet, nes-py, gym-super-mario-bros\n",
      "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n"
     ]
    }
   ],
   "source": [
    "%pip install gym-super-mario-bros==7.4.0\n",
    "%pip install tensordict==0.3.0\n",
    "%pip install torchrl==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8588,
     "status": "ok",
     "timestamp": 1733208917358,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "VzjfmUWKE6xV",
    "outputId": "bb825e38-a6a0-4f9c-9b2d-579abd4fc205"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchrl/data/replay_buffers/samplers.py:37: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. If you installed TorchRL from PyPI, please report the bug on TorchRL github. If you installed TorchRL locally and/or in development mode, check that you have all the required compiling packages.\n",
      "  warnings.warn(EXTENSION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "\n",
    "# Gym은 강화학습을 위한 OpenAI 툴킷입니다.\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# OpenAI Gym을 위한 NES 에뮬레이터\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# OpenAI Gym에서의 슈퍼 마리오 환경 세팅\n",
    "import gym_super_mario_bros\n",
    "\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfvckaOrE6xW"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1733208917359,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "qC0f36L6E6xW",
    "outputId": "2ae5bea3-9840-487e-dd12-e1c7cf995234"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "# 슈퍼 마리오 환경 초기화하기 (in v0.26 change render mode to 'human' to see results on the screen)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "else:\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
    "\n",
    "# 상태 공간을 2가지로 제한하기\n",
    "#   0. 오른쪽으로 걷기\n",
    "#   1. 오른쪽으로 점프하기\n",
    "#   2. 오른쪽 + 대시\n",
    "#   3. 오른쪽 + 점프 + 대시\n",
    "env = JoypadSpace(env, [\n",
    "    [\"right\"], [\"right\", \"A\"], [\"right\", \"B\"], [\"right\", \"A\", \"B\"]\n",
    "]) \n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPgsBK7YE6xW"
   },
   "source": [
    "### Apply Wrapper\n",
    "\n",
    "<b>\"GrayScaleObservation\"</b> : observation space를 흑백으로 변환해 색상 정보 제거\n",
    "\n",
    "<b>\"ResizeObservation\"</b> : observation space를 고정된 크기(84 X 84)로 변환\n",
    "\n",
    "<b>\"SkipFrame\"</b> : 일정 frame만큼 동일한 action을 반복 수행하여 reward 누적 후 한 번에 반환\n",
    "\n",
    "<b>\"FrameStack\"</b> : 마지막 4개의 프레임을 스택으로 쌓아 observation 데이터로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1733208917359,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "yE_tAptIE6xW"
   },
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"모든 `skip` 프레임만 반환합니다.\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"행동을 반복하고 포상을 더합니다.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # 포상을 누적하고 동일한 작업을 반복합니다.\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # [H, W, C] 배열을 [C, H, W] 텐서로 바꿉니다.\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# 래퍼를 환경에 적용합니다.\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "if gym.__version__ < '0.26':\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sI0zTkznE6xX"
   },
   "source": [
    "## Agent\n",
    "\n",
    "<b>\"Mario\"</b> 라는 에이전트를 생성합니다. 마리오는 다음과\n",
    "같은 기능을 할 수 있어야 합니다.\n",
    "\n",
    "-   **행동(Act)** : 현재 state를 기반으로 action policy에 따라 선택\n",
    "-   **기억(Remember)** : cache를 통해 메모리에 경험을 추가하고, recall을 통해 경험을 메모리에서 불러와 사용\n",
    "-   **학습(Learn)** : 최적의 action을 위한 Q-function 업데이트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733208917359,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "_htvxv8PE6xX"
   },
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"상태가 주어지면, 입실론-그리디 행동(epsilon-greedy action)을 선택해야 합니다.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cache(self, experience):\n",
    "        \"\"\"메모리에 경험을 추가합니다.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"메모리로부터 경험을 샘플링합니다.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"일련의 경험들로 실시간 행동 가치(online action value) (Q) 함수를 업데이트 합니다.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5JwDCLEE6xX"
   },
   "source": [
    "### Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733208917359,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "skQkllD5E6xX"
   },
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # 마리오의 DNN은 최적의 행동을 예측합니다 - 이는 학습하기 섹션에서 구현합니다.\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        self.net = self.net.to(device=self.device)\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # Mario Net 저장 사이의 경험 횟수\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "    주어진 상태에서, 입실론-그리디 행동(epsilon-greedy action)을 선택하고, 스텝의 값을 업데이트 합니다.\n",
    "\n",
    "    입력값:\n",
    "    state (``LazyFrame``): 현재 상태에서의 단일 상태(observation)값을 말합니다. 차원은 (state_dim)입니다.\n",
    "    출력값:\n",
    "    ``action_idx`` (int): Mario가 수행할 행동을 나타내는 정수 값입니다.\n",
    "    \"\"\"\n",
    "        # 임의의 행동을 선택하기\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # 최적의 행동을 이용하기\n",
    "        else:\n",
    "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # exploration_rate 감소하기\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # 스텝 수 증가하기\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slCewq5FE6xX"
   },
   "source": [
    "### Remember\n",
    "\n",
    "<b>cache()</b> : 경험 요소를 메모리에 저장<br />\n",
    "<b>recall()</b> : batch 크기만큼 sampling 후, 각 경험 요소를 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733208917359,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "IlcO8x3vE6xX"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):  # 연속성을 위한 하위 클래스입니다.\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
    "        self.batch_size = 64\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        입력값:\n",
    "        state (``LazyFrame``),\n",
    "        next_state (``LazyFrame``),\n",
    "        action (``int``),\n",
    "        reward (``float``),\n",
    "        done(``bool``))\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        state = first_if_tuple(state).__array__()\n",
    "        next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "        state = torch.tensor(state)\n",
    "        next_state = torch.tensor(next_state)\n",
    "        action = torch.tensor([action])\n",
    "        reward = torch.tensor([reward])\n",
    "        done = torch.tensor([done])\n",
    "\n",
    "        # self.memory.append((state, next_state, action, reward, done,))\n",
    "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        메모리에서 일련의 경험들을 검색합니다.\n",
    "        \"\"\"\n",
    "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
    "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pg1WJ4eZE6xY"
   },
   "source": [
    "## Learn\n",
    "\n",
    "<b>\"MarioNet\"</b> : *DDQN 알고리즘* 사용<br />\n",
    "<b>*DDQN 알고리즘*</b> : action의 선택(online 네트워크)과 평가(target 네트워크)를 분리\n",
    "\n",
    "- online 네트워크 : 학습을 통해 Q_online 예측\n",
    "- target 네트워크 : 일정 간격마다 online 네트워크의 가중치 복사\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733208917359,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "yTexlcNPE6xY"
   },
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    \"\"\"작은 CNN 구조\n",
    "  입력 -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> 출력\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = self.__build_cnn(c, output_dim)\n",
    "\n",
    "        self.target = self.__build_cnn(c, output_dim)\n",
    "        self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "        # Q_target 매개변수 값은 고정시킵니다.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "    def __build_cnn(self, c, output_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6MLC25oE6xY"
   },
   "source": [
    "### TD_estimate & TD_target\n",
    "\n",
    "<b>td_estimate()</b> : 현재  state에서 선택된 action에 해당하는 Q값 추출<br />\n",
    "<b>td_target()</b> :<br />1. online 네트워크로 다음 state에서 최대 Q값을 가지는 action 선택<br />\n",
    "2. target 네트워크로 최대 Q값을 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1733208917359,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "KBxtm1-WE6xY"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPY6ff6sE6xY"
   },
   "source": [
    "### Update Model\n",
    "\n",
    "<b>update_Q_online()</b> : td_estimate와 td_target 간의 loss를 계산해 가중치를 업데이트<br />\n",
    "<b>sync_Q_target()</b> : target 네트워크의 가중치를 online 네트워크와 동기화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1733208917854,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "A6HEOnsZE6xY"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "\n",
    "        # Learning rate scheduler 추가\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, step_size=10000, gamma=0.9  # 10,000 스텝마다 학습률 90%로 감소\n",
    "        )\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0QAIrAVE6xY"
   },
   "source": [
    "### Save Checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1733208917855,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "Jhn8avHDE6xY"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NVfK7JVE6xZ"
   },
   "source": [
    "### Combining Learning Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1733208917855,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "ZLBnZDvEE6xZ"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4  # 학습을 진행하기 전 최소한의 경험값.\n",
    "        self.learn_every = 5  # Q_online 업데이트 사이의 경험 횟수.\n",
    "        self.sync_every = 1e4  # Q_target과 Q_online sync 사이의 경험 수\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # 메모리로부터 샘플링을 합니다.\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # TD 추정값을 가져옵니다.\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # TD 목표값을 가져옵니다.\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # 실시간 Q(Q_online)을 통해 역전파 손실을 계산합니다.\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkVC17NJE6xZ"
   },
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1733208917855,
     "user": {
      "displayName": "헤헤",
      "userId": "03480456663930627726"
     },
     "user_tz": -540
    },
    "id": "wlO3y0B1E6xZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # 지표(Metric)와 관련된 리스트입니다.\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # 모든 record() 함수를 호출한 후 이동 평균(Moving average)을 계산합니다.\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # 현재 에피스드에 대한 지표를 기록합니다.\n",
    "        self.init_episode()\n",
    "\n",
    "        # 시간에 대한 기록입니다.\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"에피스드의 끝을 표시합니다.\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XX3VPSKsE6xZ"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xxsqyuLEE6xa",
    "outputId": "8e2316f3-5d2d-40bc-cfa1-dc0b9b5aedf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n",
      "Episode 0 - Step 228 - Epsilon 0.9999430016173364 - Mean Reward 798.0 - Mean Length 228.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 2.668 - Time 2024-12-03T03:23:18\n",
      "Episode 20 - Step 2598 - Epsilon 0.9993507107982414 - Mean Reward 636.524 - Mean Length 123.714 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 27.814 - Time 2024-12-03T03:23:46\n",
      "Episode 40 - Step 6742 - Epsilon 0.9983159194468342 - Mean Reward 712.341 - Mean Length 164.439 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 48.087 - Time 2024-12-03T03:24:34\n",
      "Episode 60 - Step 10667 - Epsilon 0.997336802286267 - Mean Reward 758.164 - Mean Length 174.869 - Mean Loss 0.133 - Mean Q Value 0.075 - Time Delta 48.082 - Time 2024-12-03T03:25:22\n",
      "Episode 80 - Step 13736 - Epsilon 0.9965718890063721 - Mean Reward 757.284 - Mean Length 169.58 - Mean Loss 0.538 - Mean Q Value 1.11 - Time Delta 46.466 - Time 2024-12-03T03:26:09\n",
      "Episode 100 - Step 17675 - Epsilon 0.9955909977615188 - Mean Reward 757.45 - Mean Length 174.47 - Mean Loss 0.685 - Mean Q Value 1.813 - Time Delta 59.584 - Time 2024-12-03T03:27:08\n",
      "Episode 120 - Step 20586 - Epsilon 0.9948667199008338 - Mean Reward 777.15 - Mean Length 179.88 - Mean Loss 0.934 - Mean Q Value 2.982 - Time Delta 44.452 - Time 2024-12-03T03:27:53\n",
      "Episode 140 - Step 24536 - Epsilon 0.9938847738077294 - Mean Reward 766.4 - Mean Length 177.94 - Mean Loss 1.178 - Mean Q Value 4.707 - Time Delta 64.488 - Time 2024-12-03T03:28:57\n",
      "Episode 160 - Step 28284 - Epsilon 0.9929539398220147 - Mean Reward 733.16 - Mean Length 176.17 - Mean Loss 1.303 - Mean Q Value 6.331 - Time Delta 57.476 - Time 2024-12-03T03:29:55\n",
      "Episode 180 - Step 32721 - Epsilon 0.9918531161836939 - Mean Reward 733.12 - Mean Length 189.85 - Mean Loss 1.2 - Mean Q Value 7.514 - Time Delta 67.838 - Time 2024-12-03T03:31:03\n",
      "Episode 200 - Step 36990 - Epsilon 0.9907951254837545 - Mean Reward 729.32 - Mean Length 193.15 - Mean Loss 1.22 - Mean Q Value 8.985 - Time Delta 65.326 - Time 2024-12-03T03:32:08\n",
      "Episode 220 - Step 41564 - Epsilon 0.9896627986473185 - Mean Reward 742.9 - Mean Length 209.78 - Mean Loss 1.245 - Mean Q Value 10.351 - Time Delta 69.566 - Time 2024-12-03T03:33:17\n",
      "Episode 240 - Step 45689 - Epsilon 0.9886427348193723 - Mean Reward 745.27 - Mean Length 211.53 - Mean Loss 1.322 - Mean Q Value 11.538 - Time Delta 63.238 - Time 2024-12-03T03:34:21\n",
      "Episode 260 - Step 49709 - Epsilon 0.9876496478563926 - Mean Reward 753.73 - Mean Length 214.25 - Mean Loss 1.42 - Mean Q Value 12.734 - Time Delta 61.154 - Time 2024-12-03T03:35:22\n",
      "Episode 280 - Step 53759 - Epsilon 0.9866501585397146 - Mean Reward 749.3 - Mean Length 210.38 - Mean Loss 1.523 - Mean Q Value 13.948 - Time Delta 61.523 - Time 2024-12-03T03:36:23\n",
      "Episode 300 - Step 57076 - Epsilon 0.9858323179374257 - Mean Reward 757.11 - Mean Length 200.86 - Mean Loss 1.607 - Mean Q Value 14.859 - Time Delta 50.709 - Time 2024-12-03T03:37:14\n",
      "Episode 320 - Step 62199 - Epsilon 0.9845705212338705 - Mean Reward 777.48 - Mean Length 206.35 - Mean Loss 1.702 - Mean Q Value 15.786 - Time Delta 78.616 - Time 2024-12-03T03:38:33\n",
      "Episode 340 - Step 64657 - Epsilon 0.9839656884267985 - Mean Reward 762.58 - Mean Length 189.68 - Mean Loss 1.783 - Mean Q Value 16.625 - Time Delta 37.444 - Time 2024-12-03T03:39:10\n",
      "Episode 360 - Step 67779 - Epsilon 0.9831980027392253 - Mean Reward 755.34 - Mean Length 180.7 - Mean Loss 1.86 - Mean Q Value 17.543 - Time Delta 47.71 - Time 2024-12-03T03:39:58\n",
      "Episode 380 - Step 69792 - Epsilon 0.9827033327642173 - Mean Reward 712.22 - Mean Length 160.33 - Mean Loss 1.887 - Mean Q Value 18.105 - Time Delta 30.724 - Time 2024-12-03T03:40:29\n",
      "Episode 420 - Step 77539 - Epsilon 0.9808019237178791 - Mean Reward 682.86 - Mean Length 153.4 - Mean Loss 2.051 - Mean Q Value 19.898 - Time Delta 36.658 - Time 2024-12-03T03:42:27\n",
      "Episode 440 - Step 80270 - Epsilon 0.9801325096679155 - Mean Reward 671.05 - Mean Length 156.13 - Mean Loss 2.08 - Mean Q Value 20.493 - Time Delta 41.674 - Time 2024-12-03T03:43:08\n",
      "Episode 460 - Step 83516 - Epsilon 0.9793374546740213 - Mean Reward 644.93 - Mean Length 157.37 - Mean Loss 2.164 - Mean Q Value 21.398 - Time Delta 49.035 - Time 2024-12-03T03:43:57\n",
      "Episode 480 - Step 87207 - Epsilon 0.9784341877335649 - Mean Reward 684.72 - Mean Length 174.15 - Mean Loss 2.239 - Mean Q Value 22.27 - Time Delta 56.074 - Time 2024-12-03T03:44:53\n",
      "Episode 500 - Step 91090 - Epsilon 0.9774848334943868 - Mean Reward 645.77 - Mean Length 159.53 - Mean Loss 2.264 - Mean Q Value 22.753 - Time Delta 59.076 - Time 2024-12-03T03:45:53\n",
      "Episode 520 - Step 95296 - Epsilon 0.976457548253283 - Mean Reward 679.2 - Mean Length 177.57 - Mean Loss 2.336 - Mean Q Value 23.58 - Time Delta 64.162 - Time 2024-12-03T03:46:57\n",
      "Episode 540 - Step 97809 - Epsilon 0.9758442813844744 - Mean Reward 671.76 - Mean Length 175.39 - Mean Loss 2.38 - Mean Q Value 24.203 - Time Delta 38.411 - Time 2024-12-03T03:47:35\n",
      "Episode 560 - Step 101005 - Epsilon 0.9750648931131956 - Mean Reward 696.88 - Mean Length 174.89 - Mean Loss 2.433 - Mean Q Value 24.738 - Time Delta 48.435 - Time 2024-12-03T03:48:24\n",
      "Episode 580 - Step 104988 - Epsilon 0.9740944553613163 - Mean Reward 696.56 - Mean Length 177.81 - Mean Loss 2.48 - Mean Q Value 25.329 - Time Delta 60.004 - Time 2024-12-03T03:49:24\n",
      "Episode 600 - Step 107999 - Epsilon 0.97336148157594 - Mean Reward 691.17 - Mean Length 169.09 - Mean Loss 2.496 - Mean Q Value 25.809 - Time Delta 45.286 - Time 2024-12-03T03:50:09\n",
      "Episode 620 - Step 112300 - Epsilon 0.9723154369924762 - Mean Reward 684.09 - Mean Length 170.04 - Mean Loss 2.491 - Mean Q Value 26.075 - Time Delta 62.123 - Time 2024-12-03T03:51:11\n",
      "Episode 640 - Step 115520 - Epsilon 0.9715330379256655 - Mean Reward 699.27 - Mean Length 177.11 - Mean Loss 2.517 - Mean Q Value 26.401 - Time Delta 48.488 - Time 2024-12-03T03:51:59\n",
      "Episode 660 - Step 119271 - Epsilon 0.9706224097429162 - Mean Reward 714.41 - Mean Length 182.66 - Mean Loss 2.497 - Mean Q Value 26.518 - Time Delta 56.654 - Time 2024-12-03T03:52:56\n",
      "Episode 680 - Step 122695 - Epsilon 0.9697919123598351 - Mean Reward 687.45 - Mean Length 177.07 - Mean Loss 2.479 - Mean Q Value 26.615 - Time Delta 51.762 - Time 2024-12-03T03:53:48\n",
      "Episode 700 - Step 127219 - Epsilon 0.968695697596079 - Mean Reward 691.04 - Mean Length 192.2 - Mean Loss 2.491 - Mean Q Value 26.791 - Time Delta 67.993 - Time 2024-12-03T03:54:56\n",
      "Episode 720 - Step 132311 - Epsilon 0.9674633323856997 - Mean Reward 698.25 - Mean Length 200.11 - Mean Loss 2.503 - Mean Q Value 26.846 - Time Delta 76.771 - Time 2024-12-03T03:56:13\n",
      "Episode 740 - Step 135382 - Epsilon 0.9667208473772506 - Mean Reward 693.43 - Mean Length 198.62 - Mean Loss 2.506 - Mean Q Value 27.11 - Time Delta 46.445 - Time 2024-12-03T03:56:59\n",
      "Episode 760 - Step 138883 - Epsilon 0.9658750950261079 - Mean Reward 676.83 - Mean Length 196.12 - Mean Loss 2.522 - Mean Q Value 27.282 - Time Delta 52.707 - Time 2024-12-03T03:57:52\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "#save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir = Path(\"checkpoints\") / \"epi10000lrchaction4_2\"\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # 게임을 실행시켜봅시다!\n",
    "    while True:\n",
    "\n",
    "        # 현재 상태에서 에이전트 실행하기\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # 에이전트가 액션 수행하기\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # 기억하기\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # 배우기\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # 기록하기\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # 상태 업데이트하기\n",
    "        state = next_state\n",
    "\n",
    "        # 게임이 끝났는지 확인하기\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if (e % 20 == 0) or (e == episodes - 1):\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXNbw_qiTpj3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": [
    {
     "file_id": "https://github.com/PyTorchKorea/tutorials-kr/blob/master/docs/_downloads/c195adbae0504b6504c93e0fd18235ce/mario_rl_tutorial.ipynb",
     "timestamp": 1733188716329
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
